\documentclass[11pt]{report}

\usepackage{a4wide}
\usepackage{todonotes}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
%\setlength{\parindent}{0pt}

\newtheorem{algorithm}{Algorithm}
\newtheorem{subroutine}{Subroutine}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{explanation}{Explanation}
\theoremstyle{remark}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\renewcommand{\phi}{\varphi}
\newcommand{\e}{\varepsilon}
\newcommand{\T}{\mathcal{T}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\hp}{{hp}}
\DeclareMathOperator{\cp}{copy}
\renewcommand{\d}{\Delta}
\newcommand{\node}{\Delta}
\renewcommand{\e}{\epsilon}
\newcommand{\he}{\hat e}
\newcommand{\te}{\tilde e}
\newcommand{\sgn}{\operatorname{sgn}}

\begin{document}

\title{}
\author{Jan Westerdiep}
\maketitle

Consider a compact domain $D \subset \R$, and a function $f: D \to \R$. Our goal is to \emph{quickly} find \emph{good} (piecewise) polynomial approximations to $f$. This immediately raises questions: what constitutes a good approximation, when is finding it considered quick, and how will we partition our domain? These are all valid questions which will be answered.

\part{Theory}
\setcounter{chapter}{-1}
\chapter{Preliminaries}

\section{Norms}
\label{sec:norms}
\section{Lebesgue spaces}
\label{sec:leb}

\section{Limiting behaviour of functions: $\mathcal{O}$}

\section{Trees}
\begin{definition}
A \emph{tree} is a connected graph $G = (T,E)$ without any cycles. A \emph{tree structure} is a way of representing the hierarchical nature of a structure in graphical form.

A vertex in a tree is often called a \emph{node}. Sometimes, a special node exists -- the \emph{root node}, $D$. Trees with root nodes are called \emph{rooted trees}.
\end{definition}
\begin{definition}
  In a rooted tree, each node has a \emph{depth} $d$ -- the length of the path to the root. Given a node $\node \not= D$, one can define the \emph{parent} to be the node $\node^+ \in T$ with $d(\node) = d(\node^+) + 1$ such that $\{\node, \node^+\} \in E$. On the other hand, the \emph{children} of $\node$ are all nodes that have $\node$ as their parent. Nodes that share a parent are called \emph{siblings}. Nodes without children are called \emph{leaves} (with $\L(T)$ denoting the set of leaves of $T$) and nodes that are not leaves are called \emph{inner nodes} (with $\I(T)$ the set of inner nodes).
\end{definition}
\begin{lemma} Every node except for $D$ has a parent. \end{lemma}
\begin{remark}
  A rooted tree immediately implies a certain hierarchy in which nodes with equal depth have equal rank.
\end{remark}

With the notion of trees properly introduced, we can now further enlarge our toolset.
\begin{definition}
  A \emph{subdivision rule} $s$ is a non-node-specific way to partition a given polygon $\node \subset D$ into $2$ elements.
\end{definition}
\begin{example}
  Let $D = [0,1] \subset \R$. One subdivision rule is to divide an interval in half: $[a,b]$ becomes $\{[a,\frac{a+b}{2}], [\frac{a+b}{2}, b]\}$.
\end{example}

Given a subdivision rule and a domain $D$, one can recursively construct an (infinite) tree $\T^*$ with root node $D$. In each step, take the set of leaves $\L(T_n)$ and create $T_{n+1} = T_n \sqcup s(\L(T_n))$. Taking the limit to infinity yields $\T^*$.

All trees generated by iterated use of the subdivision rule are subsets\footnote{Strictly speaking, a tree is an ordered pair $(T, E)$. Because the edges are in our case easy to determine (by the implied parent-child relation), we will loosen our terminology and say that a tree \emph{equals} its set of vertices $T$, in which case $T \subset \T^*$ implies that the assertion holds.} of $\T^*$, and they all share a trait.
\begin{lemma}
  \label{lem:part}
  For a tree $T$ generated by iterated use of the subdivision rule, it holds that $\L(T)$ -- the set of leaves of $T$ -- is a partition of $D$.
\end{lemma}
\begin{definition}
  It is useful to point out that trees $T$ of the above type can create \emph{adaptive} partitions of $D$. This means that in some regions of $D$, a higher concentration of elements might be present. This means that we can \emph{adapt} our generation algorithm to the problem at hand, creating a well-suited partition for this particular problem.
\end{definition}

\section{Polynomial approximation in one dimension}
\label{sec:polyapprox}
Given an interval $D = [a,b] \subset \R$ and a function $f: D \to \R$, one might wonder how to find good approximations to $f$ within some function class. This question has no single answer, as both the space and the notion of a good approximation is as of yet undefined. One commonly used tactic to quantify a good approximation is to find a norm well-suited to your intentions.

To further narrow our search, we will only use a very special class of functions with interesting properties.
\begin{definition}
  A polynomial is a function $p: \R \to \R$ of the following form:
  \[
    p(x) = \sum_{k=0}^n a_k x^k.
  \]
  The values $a_k$ are the \emph{coefficients} of $p$, and the largest power of $x$ with nonzero coefficient is called the \emph{degree} of $p$. The set of all polynomials is denoted by $\P$. When constrained to a domain $D \subset \R$, $\P(D) := \{ q_{|D}: q \in \P \}$ denotes the set of all polynomials from $D$ to $\R$.
\end{definition}

These polynomials are useful instruments in dealing with function approximation. Given an interval $D = [a,b] \subset \R$ and a Lebesgue space $L^p(D)$, with $1 \leq p \leq \infty$, the following results hold.
\begin{theorem}
  For $1 \leq p < \infty$, the set $\P(D)$ is dense in $L^p(D)$.
\end{theorem}
\begin{proof}
  Our interval $D$ is closed and bounded, thus by \cite[Thm.~1.35]{RY} compact. Then by \cite[Thm.~1.40]{RY}, the set $\P(D)$ is dense in $C(D)$, the set of continuous functions from $D$ to $\R$. Lastly, by \cite[Thm.~1.62]{RY}, $C(D)$ is dense in $L^p(D)$. Transitivity of denseness implies the result.
\end{proof}
\begin{theorem}[{\cite[Thm.~8.1]{NA}}]
  \label{thm:weierstrass}
  The set $\P(D)$ is dense in $C(D)$ with respect to the $\infty$-norm $\|\cdot\|_\infty$.
\end{theorem}
\begin{remark}
  In the above theorem, we deliberately did not state that $\P(D)$ lies dense in $L^\infty(D)$! This would imply that $\overline{C(D)} = L^\infty(D)$. To see this cannot be the case, consider $D = [-1,1], f(x) = \sgn(x)$. Any continuous function $g$ with $\|f - g\|_\infty < \frac{1}{3}$ must have $g(x) < f(x) + \frac{1}{3} = -\frac{2}{3}$ for $x < 0$. Analogously, $g(x) > \frac{2}{3}$ for $x > 0$. By continuity, $g(0)$ must satisfy both $g(0) \leq -\frac{2}{3}$ and $g(0) \geq \frac{2}{3}$. Such a function cannot exist, and so we cannot get arbitrarily ``close'' to $f$ using only continuous functions.
\end{remark}

These two results help to show the vast importance of polynomial functions. The above norms are all interesting, with two elements standing out. The first is (perhaps) the most intuitive way of defining two functions to be ``close'', and the second holds a very special property.

\subsection{$\infty$-norm}
Given an $\e > 0$ and a continuous $f$, we found with Theorem~\ref{thm:weierstrass} that it is possible to find a polynomial $p$ such that
\[
  \|f - p\|_\infty := \sup \{ |(f-p)(x)|: x \in D \} < \e.
\]
If we however restrict ourselves to just polynomials of degree $n$ or less (denoted by $\P^n$), this result no longer holds. It is therefore interesting to look at the lowest value that \emph{is} obtainable, or more precisely:
\begin{gather}
  \text{Given $f \in C(D)$ and $n \geq 0$, find $p_n \in \P^n$ such that} \nonumber\\
  \|f - p_n\|_\infty = \inf_{\substack{q \in \P^n}}\|f - q\|_\infty.
  \label{minimax}
\end{gather}
\begin{theorem}[{\cite[Thm.~8.2]{NA}}]
  \label{thm:minimaxexists}
  The equality in \eqref{minimax} is attained for a polynomial of degree $n$, i.e.~the infimum is a minimum.\todo{Stelling 3.32 uit LinAna boek is zelfs nog breder: elke niet-lege convexe deelverz voldoet}
\end{theorem}
Because this $p_n$ \emph{minimizes} the \emph{maximal} absolute value of $f(x) - q(x)$, this polynomial is often referred to as the \emph{minimax polynomial}.
\begin{theorem}[{\cite[Thm.~8.5]{NA}}]
  There is exactly one minimax polynomial $p_n$ of degree $n$ for every $f$ on $D$.
\end{theorem}

While we haven't shown the proof of Theorem~\ref{thm:minimaxexists} here, the reader is invited to read it and conclude that this proof is not \emph{constructive} in the sense that it gives no \emph{algorithm} to find the minimax polynomial. This makes for lesser practical use, as an implementation would like to actually \emph{find} this polynomial. The most widely used solution is an \emph{iterative} algorithm\footnote{An iterative algorithm creates a sequence of improving approximate solutions to some problem.} -- the Remez Algorithm~\cite{remez} -- that can find minimax approximations under certain conditions such as continuity of $f$. However, we will not look into this in more detail.

\section{$2$-norm}
As stated earlier in \S\ref{sec:leb}, the space $L^2$ of twice-integrable functions holds a very special property, in the sense that it is the only Hilbert space of this family. \todo{uitleggen waarom dit interessant is: orthogonaliteit enzo} As earlier, our set of polynomials $\P$ is dense in $L^2(D)$. When we confine ourselves to using polynomials of degree $n$ or less, we get an analogous problem:
\begin{gather}
  \text{Given $f \in L^2(D)$ and $n \geq 0$, find $p_n \in \P^n$ such that} \nonumber\\
  \|f - p_n\|_2 = \inf_{\substack{q \in \P^n}}\|f - q\|_2.
  \label{approx2norm}
\end{gather}
\begin{theorem}[{\cite[Thm.~9.2]{NA}}]
  \label{thm:2norm}
  Given $f \in L^2(D)$, there exists a unique polynomial $p_n \in P^n$ such that $\|f - p_n\|_2 = \inf_{q \in P^n} \|f - q\|_2$.
\end{theorem}

Contrasting the previous: in the $L^2$ case, we can easily find this \emph{polynomial of best approximation} $p_n(x) = c_0 + \ldots + c_nx^n$.

\begin{proof}[Proof (of Theorem~\ref{thm:2norm})]
Since the $2$-norm is nonnegative and $\R_+ \ni \xi \mapsto \xi^{1/2}$ is monotonic increasing, we can also minimise $\|f - p_n\|_2^2$ instead:
\begin{align}
  \label{proof2normmin}
  \|f - p_n\|^2_2 &= \int_0^1[f(x) - p_n(x)]^2 dx \nonumber \\
  &= \int_0^1 [f(x)]^2 dx - 2\sum_{j=0}^n c_j \int_0^1 f(x) x^j dx + \sum_{j=0}^n\sum_{j=0}^n c_j c_k \int_0^1 x^{k+j} dx.
\end{align}
When viewing $\|f - p_n\|_2^2$ as a function of $c_0, \ldots, c_n$ to $\R$, attaining the minimum implies that the gradient is zero. Therefore, its partial derivative wrt.~$c_l$ must be zero as well:
\[
  0 = \frac{\partial \|f - p_n\|_2^2}{\partial c_l} = - 2 \int_0^1 f(x) x^l dx + 2 \sum_{k=0}^n c_k \int_0^1 x^{k+l} dx
\]
so
\begin{equation}
  \label{proof2normc}
  \int_0^1 f(x) x^l dx = \sum_{k=0}^n c_k \int_0^1 x^{k+l} dx
\end{equation}
for all $l$. This leads to a system of $n+1$ linear equations:
\[
  \sum_{k=0}^n M_{jk} c_k = b_j, \quad j = 0, \ldots, n, \quad M_{jk} = \int_0^1 x^{k+j} dx = \frac{1}{k+j+1}, \quad b_j = \int_0^1 f(x) x^j dx.
\]
The matrix $M_{jk}$ has nonzero determinant \cite{choi}, so this system has a unique solution $(c_0, \ldots, c_n)$.
\end{proof}

The matrix $M_{jk}$ is often referred to as the Hilbert matrix. There is however a slight problem with this construction: as we are solving a linear system, we are effectively computing the inverse of the Hilbert matrix. This inverse has \emph{huge} elements and even for only slightly large $n$, is not able to be represented by 64-bit integers. To overcome this problem, we will look at another method, namely \emph{orthogonal polynomials}.

\subsection{Orthogonal polynomials}
\begin{definition}
  A sequence of polynomials $\{ \phi_j: j = 0, 1, \ldots \}$ is called a \emph{system of orthogonal polynomials} on $(a,b)$ if each $\phi_j$ is of degree $j$ and
  \[
    \int_a^b \phi_k(x) \phi_j(x) dx = 0 \text{ if } k \not= j.
  \]
  If, in addition, this integral equals $\delta_{jk}$, then the system is called \emph{orthonormal}.
\end{definition}

We will show that a system of orthogonal polynomials exists on any interval $(a,b)$.

Let $\phi_0(x) = 1$ and suppose $\phi_j$ has already been constructed for $j$ up to $n \geq 0$. Then
\[
  \int_a^b \phi_k(x) \phi_j(x) dx = 0, \quad k \in \{0,\ldots,j-1,j,\ldots,n\}.
\]
Define
\[
  q_{n+1}(x) = x^{n+1} - a_0 \phi_0(x) - \cdots - a_n \phi_n(x), \quad a_j = \frac{\int_a^b x^{n+1} \phi_j(x) dx}{\int_a^b \phi_j^2(x) dx}.
\]
It follows that
\begin{align*}
  \int_a^b q(x) \phi_j(x) dx &= \int_a^b x^{n+1} \phi_j(x) dx - a_j \int_a^b \phi_j^2 (x) dx \\
  &= 0, \quad 0 \leq j \leq n
\end{align*}
by using orthogonality of the sequence $\phi_j$. With this choice of $a_j$ we have ensured that $q_{n+1}$ is orthogonal to all previous members of the sequence, and $\phi_{n+1}$ can be defined as any nonzero multiple of $q_{n+1}$. This procedure is called \emph{Gram-Schmidt orthogonalisation}.
\begin{theorem}
  Given an orthogonal sequence $\{\phi_j\}$ over $(a,b)$ and a function $f \in L^2(a,b)$, the polynomial
  \[
    p_n(x) = \gamma_0 \phi_0(x) + \cdots + \gamma_n \phi_n(x), \quad \gamma_n = \frac{\int_a^b f(x) \phi_j(x) dx}{\int_a^b \phi_j^2 (x) dx}
  \]
  is the unique polynomial of best approximation of degree $n$ to $f$.
\end{theorem}
\begin{remark}
  We conclude that the hurdle of inverting the Hilbert matrix is now overcome. In fact, we do not even have to invert \emph{any} matrix any more and rely solely on the sequence of orthogonal polynomials.\todo{je inverteert impliciet wel een matrix. uitzoeken}
\end{remark}
\subsection{Legendre polynomials}
We wish to create an orthonormal sequence for the interval $(-1,1)$. Beginning with $P_0(x) = 1$, we find $P_1(x) = q_1(x) = x$. Continuing on, we find 
\begin{align*}
  P_0(x) &= 1, \\
  P_1(x) &= x, \\
  P_2(x) &= \frac{1}{2}(3x^2-1), \\
  P_3(x) &= \frac{1}{2}(5x^3 - 3x).
\end{align*}
These polynomials are called the \emph{Legendre polynomials} and play a main role in the implementation of the \emph{polynomial of best approximation}.

Replacing $x$ by
\[
  \frac{2}{b-a} x + \frac{a+b}{a-b}
\]
yields a sequence of orthogonal polynomials $\{ \tilde P_n \}$ over $(a,b)$ which we will call the \emph{shifted Legendre polynomials}.

The following results will be used in the implementation.
\begin{theorem}[{\cite[p.~743]{legen}}]
  The Legendre polynomials $P_n$ satisfy the following recurrence relation:
  \[
    (n+1) P_{n+1}(x) = (2n+1) x P_n(x) - n P_{n-1}(x)
  \]
\end{theorem}
\begin{theorem}[{\cite[Ex.~3.28]{RY}}]
  The $n$th Legendre polynomial satisfies the following equality:
  \[
    \int_{-1}^1 P_n(x)^2 dx = \frac{2}{2n+1}.
  \]
  More generally, the $n$th shifted Legendre polynomial satisfies
  \[
    \int_a^b \tilde P_n(x)^2 dx = \frac{b-a}{2n+1}.
  \]
\end{theorem}


\chapter{One-dimensional tree generation}

Given $r$ fixed, say we want to approximate a given function $f$ over some interval $D \subset \R$. The previous sections provided us with two tools: firstly, with the subdivision rule $s$ in hand, we can generate a plethora of partition trees $T$ (recall that the leaves $\L(T)$ of $T$ must always partition $D$ -- see Lemma~\ref{lem:part}). On each element of this partition, we can approximate $f$ by a polynomial in $\P^r$ to create a piecewise polynomial approximation on $D$.

On each of these leaves $\node$ in $\L(T)$, we can find the polynomial of best approximation $p_\node$, given that we `know' how to find such a polynomial. The next definition specifies this.

\begin{definition}
  \label{def:functional}
  The function $e: \T^* \to \R_{\geq 0}$ is a function assigning the best obtainable error by approximating $f$ on $\node \in \T^*$ with functions in $\P^r$. If this function further satisfies the \emph{subadditivity rule}
  \[
    e(\node) \geq e(\node') + e(\node''), \quad \{\node', \node''\} = s(\node),
  \]
  then it is called an \emph{error functional}.
\end{definition}
\begin{example}
  A common choice of $e$ is 
  \[
    e(\node) := \|f - p_\node\|^2_{2,\node} = \min_{q \in \P^r} \|f-q\|^2_{2,\node},
  \]
  the square of the difference in $2$-norm of $f$ with the best polynomial approximation over $\node$. This square is needed to ensure satisfaction of the subadditivity rule.
\end{example}

\begin{definition}
Given an error functional $e$, we can define the \emph{total error} $E(T)$ of a tree as
\[
  E(T) := \sum_{\node \in \L(T)} e(\node).
\]
\end{definition}
\begin{remark}
Note that because of the subadditivity rule, it must hold that
\[ T_1 \supset T_2 \implies E(T_1) \leq E(T_2). \]
This means that we can never increase our total error subdividing a node.
\end{remark}

Given the functionals $e$ and $E$ and $m \geq 1$, consider the class $\T_m$ of all trees $T$ generated from $D$ with $\#\I(T) \leq m$ inner nodes. Define
\[
  E_m := \min_{T \in \T_m} E(T)
\]
as the best obtainable error by $m$ subdivisions. Of course, this error can be explicitly found by considering all $\O(2^m)$ trees in $\T_m$. This is however an exponential problem and therefore not suitable for real-life problems.

In the following, we will look at several algorithms that (try to) tackle this problem. Each algorithm will operate in the same way: we have a tree $T_j$ with $\#\I(T_j) = n$ inner nodes, and want to find which leaves to subdivide in order to generate $T_{j+1}$.

\section{The naive approach}
With our error functional $e$ and leaves in place, why not just subdivide the leaves where the error is maximal? It might seem like a valid point at first.

\begin{algorithm}[Greedy]
  \label{alggreedy}
  For $j=0$, $T_0 = D$ is the root node of $\T^*$. If $T_j$ has been defined, examine all leaves $\node \in \L(T_j)$ and subdivide the leaves $\node$ with largest $e(\node)$ to produce $T_{j+1}$. In case of multiple candidates, subdivide all.
\end{algorithm}

This algorithm is called \emph{greedy}, for it makes the best \emph{local} choice, rather than considering the \emph{global} problem at hand.

\begin{example}
Take a look at the function
\[
  f(x) = \begin{cases} \sin(16 \pi x) & x \in [0,1] \\ 0 & x \in [1,1\frac{1}{2}] \\ 1 & x \in [1\frac{1}{2},2] \end{cases}.
\]
We will be approximating $f$ with polynomials of degree $0$ using the standard error functional $\|f - p_n\|_2^2$. The optimal subdividing strategy immediately subdivides the right half of the domain, effectively nulling the total error there. The greedy algorithm, will (without any effect whatsoever) subdivide the first half into tiny pieces before considering the right half.
\todo{dit stuk beter}
\end{example}

\section{$h$-tree generation: Binev 2004}
The above algorithm shows a simple concept: adaptive refinement of a partition. The tree this approach generates is thusly called an $h$-tree for it refines $h$, the mesh grid size.

Instead of looking purely at the error functional $e$ in Algorithm~\ref{alggreedy}, a \emph{modified error functional} is introduced in \cite{2004}. This modified error functional $\te$ penalizes nodes that don't improve after a subdivision. This modification makes for provable performance enhancements, as we will shortly see.

Let $\te(D) := e(D)$. Then, given that $\te(\node)$ is defined, let $\te$ for each child $\node_j$ of $\node$ be as follows:
\begin{equation}
  \label{eqn2004}
  \te(\node_j) := q(\node), \quad q(\node) := \frac{\sum_{j=1}^2 e(\node_j)}{e(\node) + \te(\node)} \te(\node).
\end{equation}
\begin{algorithm}[Binev2004{\cite[p.~204]{2004}}]
  \label{alg:2004}
  For $j=0$, $T_0 = D$ is the root node of $\T^*$. If $T_j$ has been defined, examine all leaves $\node \in \L(T_j)$ and subdivide the leaves $\node$ with largest $\te(\node)$ to produce $T_{j+1}$. In case of multiple candidates, subdivide all.
\end{algorithm}
\begin{theorem}[{\cite[Thm.~5.2]{2004}}]
  \label{thm:2004}
  There is an absolute constant $C > 0$ such that for each $j$, the output tree $T_j$ of Algorithm~\ref{alg:2004} satisfies $E(T_j) \leq C E_m$ when $m \leq n/6$. Furthermore, the algorithm uses up to $(n + 1)C$ arithmetic operations and computations of $e$ to find this $T_j$.
\end{theorem}
\begin{theorem}[{\cite[Thm.~2]{2007}}]
  \label{thm:2004C}
  The constant $C$ in the above theorem is equal to
  \[
    C = 1 + \frac{2( m + 1)}{n + 1 - m}.
  \]
\end{theorem}

\subsection{A better modified error functional: Binev 2007}
Algorithm~\ref{alg:2004} can be improved by replacing the definition of the modified error in~\eqref{eqn2004} in the following way. Let $\te(D) := e(D)$ still, but for each child $\node_j \in \C(\node)$, let
\[
  \te(\node_j) := \left( \frac{1}{e(\node_j)} + \frac{1}{\te(\node)}\right)^{-1}.
\]
The accompanying algorithm is now an exact restatement of the earlier result.
\begin{algorithm}[Binev2007{\cite{2007}}]
  \label{alg:2007}
  For $j=0$, $T_0 = D$ is the root node of $\T^*$. If $T_j$ has been defined, examine all leaves $\node \in \L(T_j)$ and subdivide the leaves $\node$ with largest $\te(\node)$ to produce $T_{j+1}$. In case of multiple candidates, subdivide all.
\end{algorithm}
\begin{theorem}[{\cite[Thm.~4]{2007}}]
  \label{thm:2007}
  Using terminology of Theorem~\ref{thm:2004}: at each step of Algorithm~\ref{alg:2007}, the output tree $T$ satisfies
  \[
    E(T) \leq \left( 1 + \frac{m+1}{n + 1 - m} \right) E_m,
  \]
  whenever $m \leq n$.
\end{theorem}
\begin{remark}
  Comparing Theorems~\ref{thm:2004} and \ref{thm:2004C} with the above, we conclude that the new algorithm gives better approximations.
\end{remark}

\subsection{Ensuring linear complexity}
Algorithms~\ref{alg:2004} and~\ref{alg:2007} have linear complexity in the sense that the amount of operations needed to find $T_j$ is linear in the amount of subdivisions (or equivalently, the amount of inner nodes $n$). However, sorting a list to find a certain element -- in our case, finding the node with largest (modified) error -- is inherently a problem of complexity $\O(n \log n)$. 

To overcome this problem, it is suggested to use binary bins: for each node, find an integer $\kappa$ such that $2^\kappa \leq \te(\node) < 2^{\kappa + 1}$ and place $\node$ in bin $\kappa$. The highest nonempty bin now becomes the set of nodes to subdivide. Theorem~\ref{thm:2004} still holds true in this case \cite[p.~207]{2004}.

\section{$hp$-tree generation: Binev 2013}
Adaptive approximation by piecewise polynomials can be generalized in different ways. One of the most investigated forms of it is the $hp$-approximation in which the local size of elements of the partition and the degree of the polynomials may vary, but the total number of degrees of freedom is controlled.

One motivation for this generalization is that sufficiently smooth functions are best approximated by high-order polynomials, while $h$-refinement is better suited for non-smooth functions. \todo{literatuur} To profit from both sides, Binev introduced an $hp$-tree generating algorithm.

Given a tree $T$, we define an $hp$-tree $T^{hp}$ by decorating each leaf $\node \in \L(T)$ with polynomial space $\P^{r(\node)}$ of a \emph{node-specific} degree $r(\node)$. The number of degrees of freedom in this tree is now
\[
  \cN(T^{hp}) = \sum_{\node \in \L(T)} r(\node).
\]

Of course, we want to approximate some function $f$. To accomplish this, we will now develop a framework to work with.
\subsection{Needed definitions}
Recall the error functional $e$ from Definition~\ref{def:functional}. We will extend this to the following.
\begin{definition}
  The function $e_k$ is a function
  \[
    e_k: \N \times \T^* \to \R_{\geq 0}
  \]
  assigning the best obtainable error by approximating $f$ on $\node \in \T^*$ with functions in $\P^k$.

  If this function further satisfies the subadditivity rules 
  \[ 
    e_k(\node) \geq e_{k+1}(\node) \qquad \text{and} \qquad e_1(\node) \geq e_1(\node') + e_1(\node''), \quad \{\node', \node''\} = s(\node),
  \]
  then this function is called the \emph{local error of approximation}.
\end{definition}
\begin{definition}
  The \emph{total error} of an $hp$-tree $T^\hp$ is defined as
  \[
    E(T^\hp) := \sum_{\node \in \L(T^\hp)} e_{r(\node)}(\node).
  \]
\end{definition}
\begin{definition}
  Given a function $f$, the best $hp$-approximation with up to $n$ degrees of freedom is defined as
  \[
    E^\hp_n := \inf_{\cN(T^\hp) \leq n } E( T^\hp).
  \]
\end{definition}

The goal is to find a coarse-to-fine algorithm that analyzes the errors at the current tree and decides how to define the next tree with the degrees of freedom increased by one.

\begin{definition}
  \todo{Dit heeft nog geen eenduidige definitie}

  Let $T^h_N$ with root node $D$ and number of leaves $\#\L(T^h_N) = N$.
\end{definition}
\begin{definition}
  For each $\node \in T^h_N$, define $T^h_N(\node)$ as the maximal subtree of $T^h_N$ with root node $\node$.
\end{definition}
\begin{definition}
  The order $r(\node)$ of a node in $T^h_N$ is defined as number of leaves of $T^h_N(\node)$: $r(\node) := \#\L(T^h_N(\node))$.
\end{definition}
\begin{definition}
  Define the modified error functional $\te^h$ as follows: $\te^h(D) := e_1(D)$, and for each non-root node $\node$ with children $\node'$:
  \[
    \te^h(\node') := \left( \frac{1}{e_1(\node')} + \frac{1}{\te^h(\node)} \right)^{-1}.
  \]
\end{definition}

At each step of the (to be stated) Binev2013 algorithm, we will start with a tree $T = T^h_N$ and gradually `trim' it to generate a new tree.
\begin{definition}
  Given $T$ and $\node \in \I(T)$, we define the \emph{total error of $\node$} to be
  \[
    E_T^\hp( \node) := \sum_{ \node' \in \L(T_N^h(\node) \cap T)} e_{r(\node')}(\node').
  \]
\end{definition}
With these formulas in hand, we can now formulate Binev2013.
\subsection{The Algorithm}
\begin{algorithm}[Binev2013\cite{2013}]
  \label{alg:2013}
  Create an initial tree $T^h_1$ with one node, namely $D$. Then, in each step $N$ of the algorithm:
  \begin{enumerate}
    \item Set $T = T^h_N$. \label{en:2013setT}
    \item For each leaf $\node \in \L(T)$, set $\he( \node) = \te^h(\node)$.\label{en:2013setleaf}
    \item For each inner node $\node \in \I(T)$ -- using a reverse level-order traversal -- if ${e_{r(\node)}(\node) < E_T^\hp(\node)}$:\label{en:2013setinner}
      \begin{enumerate}
        \item Make $\node$ a leaf of $T$ by removing its children.\label{en:2013trim}
        \item For each leaf $\node' \in \L(T^h_N(\node))$, set $\he(\node') = \he(\node') \cdot \frac{e_{r(\node)}(\node)}{E_T^\hp(\node)}$. \label{en:2013mult}
      \end{enumerate}
    \item Create $T^\hp_N$ from $T$ by giving each leaf $\node \in \L(T)$ degree $r(\node)$.\label{en:2013create}
    \item Subdivide the leaves $\node$ with highest $\he(\node)$, thus creating $T^h_{N+1}$.\label{en:2013subd}
  \end{enumerate}
\end{algorithm}
\begin{explanation} The above algorithm can be motivated by the following. \todo{uitleg moven naar eigen ding? en misschien preciezer zijn?}
\begin{enumerate}
  \item We create a new temporary tree $T$ as to not edit $T^h_N$ (which we still need intact for Step~\ref{en:2013create}). 
  \item An initial value for each leaf is then set, which we will be multiplying in Step~\ref{en:2013mult}.
  \item We traverse $\L(T)$ by means of a reverse level-order traversal (in other words, level-by-level, fine-to-coarse) to comply with the definition of $E^\hp_T(\node)$. 
    
    Of course, we are trying to minimize the total error. The condition ensures that we make the optimal choice concerning $\node$: either we assign $r(\node)$ to $\node$ (i.e., we refine $p$), or do nothing (i.e., refine $h$). 
    
    In the do-nothing case, the fine-to-coarse traversal ensures that we have already found a good refinement strategy\todo{beter verwoorden?} for $\node$ and its children. Else,
    \begin{enumerate}
      \item we trim the tree, and 
      \item the modified error of descendant leaves of $\node$ is multiplied by a value in $[0,1)$ (i.e.~lowered) to lessen the likelihood to be subdivided.\todo{Waarom precies deze multiply?}
    \end{enumerate}
  \item We then use the original $T_N^h$ to see which nodes have been merged. 
  \item This step is of course the induction step.
\end{enumerate}
\end{explanation}

\begin{theorem}[{\cite{2013}}]
  Let $T^\hp_N$ be the $hp$-tree found by Algorithm~\ref{alg:2013}. Then, for $n \leq N$ we have
  \[
    E(T^\hp_N) \leq \left( 1 + \frac{ 2(n-1)}{ N + 1 - n} \right) E^\hp_n.
  \]
  Assuming that the calculation of $e_k(\node)$ requires $\O(1)$ operations (i.e., does not depend on $k$), the complexity of Algorithm~\ref{alg:2013} is at most $\O(N^2)$.
\end{theorem}
\begin{remark}
  Comparing this result with Theorem~\ref{thm:2007} makes it seem as if the new algorithm is worse in both terms of complexity and upper bound. However, looks are deceiving, as Algorithm~\ref{alg:2013} can create $hp$-trees whereas Algorithm~\ref{alg:2007} creates $h$-trees. The big difference is that we are now able to approximate a much larger class of functions.\todo{Stelling/stuff hierover vinden}
\end{remark}

\begin{thebibliography}{9}
  \bibitem{RY}
  Bryan Rynne \and M.A.~Youngson, \emph{Linear Functional Analysis}, Springer, 2008.
  \bibitem{NA}
  Endre S\"uli \and David Mayers, \emph{An Introduction to Numerical Analysis}, Cambridge University Press, 2003.
  \bibitem{2004} Peter Binev and Ronald DeVore, \emph{Fast computation in adaptive tree approximation}, Springer-Verlag, 2004.
  \bibitem{2007} Peter Binev, \emph{Adaptive Methods and Near-Best Tree Approximation}, Oberwolfach Report 29, 2007.
  \bibitem{choi} M.D. Choi, \emph{Tricks or Treats with the Hilbert Matrix}, Amer. Math. Monthly 90, 301-312, 1983. 
  \bibitem{2013} Peter Binev, \emph{Instance optimality for hp-type approximation}, Oberwolfach Report 39, 2013.
  \bibitem{remez} R.M.~Aarts, C.~Bond, P.~Mendelsohn and E.W.~Weisstein, \emph{Remez Algorithm}, MathWorld, \url{http://mathworld.wolfram.com/RemezAlgorithm.html}.
  \bibitem{legen} George B. Arfken, Hans J. Weber, \emph{Mathematical Methods for Physicists}, Elsevier Academic Press, 2005.
\end{thebibliography}

\end{document}
